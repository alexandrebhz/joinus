ON this file we gonna describe how the crawlers will work this will have a interface, where the user can crud sites, each sites we point the url that contains the jobs, the crawler will need to know how to paginate, how to verify if that has been already inserted in the local db also, for each field needs to have a path (html) of how to get that, the core part will be scheduler to check each site based in a cron style so could be once a day, or once a week that would be changed based on the site, at some point the local db will make a sync with the back end api that will receive the new jobs that has been found so far